# ===========================================
# Hollon-AI Environment Configuration
# ===========================================

# Node Environment
NODE_ENV=development

# ===========================================
# Database (PostgreSQL with pgvector)
# ===========================================
# Database connection settings
DB_HOST=localhost
DB_PORT=5432
DB_NAME=hollon
DB_USER=hollon
DB_PASSWORD=your_secure_password_here

# Database schema (optional, defaults based on environment)
# Production/Development: hollon
# Test: hollon_test (automatically suffixed with worker ID in parallel tests)
DB_SCHEMA=hollon

# Full connection URL (alternative to individual settings)
DATABASE_URL=postgresql://${DB_USER}:${DB_PASSWORD}@${DB_HOST}:${DB_PORT}/${DB_NAME}

# Connection pool configuration
# Maximum number of clients in the pool (default: 10)
DB_POOL_SIZE=10

# Maximum time (ms) a client can be idle before being closed (default: 30000)
DB_IDLE_TIMEOUT_MS=30000

# Maximum time (ms) to wait for a connection from the pool (default: 10000)
DB_CONNECTION_TIMEOUT_MS=10000

# Maximum lifetime (ms) of a connection in the pool (default: 1800000 = 30 minutes)
DB_MAX_LIFETIME_MS=1800000

# ===========================================
# pgvector Extension Settings
# ===========================================
# Enable pgvector extension for vector similarity search
# Set to 'true' to enable semantic search capabilities across the application
# Set to 'false' to disable vector operations (falls back to traditional search)
# Default: true
PGVECTOR_ENABLED=true

# -------------------------------------------
# HNSW Index Parameters (Recommended)
# -------------------------------------------
# HNSW (Hierarchical Navigable Small World) is the recommended index type for most use cases
# Provides fast approximate nearest neighbor search with excellent recall
# Best for: Production workloads, large datasets (>100K vectors), low-latency queries
#
# Trade-offs:
# - Pros: Excellent query performance, high recall, scales well
# - Cons: Slower index build, higher memory usage during construction

# HNSW M Parameter: Maximum bidirectional links per layer
# Range: 4-64 (default: 16, recommended: 16-32 for most cases)
# - Lower values (4-8): Faster builds, less memory, lower recall
# - Medium values (16-24): Balanced performance (recommended for most use cases)
# - Higher values (32-64): Better recall, more memory, slower builds
# Memory impact: ~(M * 2 * 4 bytes) per vector
PGVECTOR_HNSW_M=16

# HNSW ef_construction: Dynamic candidate list size during index build
# Range: 4-1000 (default: 64, recommended: 64-200)
# - Lower values (4-32): Faster index builds, lower quality
# - Medium values (64-100): Good balance (recommended)
# - Higher values (200-1000): Slower builds, better recall
# Build time increases linearly with this value
# Recommended: Start with 64, increase to 128-200 for critical applications
PGVECTOR_HNSW_EF_CONSTRUCTION=64

# HNSW ef_search: Dynamic candidate list size during search
# Range: 1-1000 (default: 40, recommended: 40-100)
# - Lower values (1-20): Faster queries, lower recall
# - Medium values (40-60): Good balance (recommended)
# - Higher values (100-1000): Better recall, slower queries
# Note: This can be overridden per-query in application code
# Query time increases with this value (typically logarithmically)
PGVECTOR_HNSW_EF_SEARCH=40

# -------------------------------------------
# IVFFlat Index Parameters (Alternative)
# -------------------------------------------
# IVFFlat (Inverted File with Flat compression) for smaller datasets or faster builds
# Uses k-means clustering to partition the vector space
# Best for: Smaller datasets (<100K vectors), development, faster index builds
#
# Trade-offs:
# - Pros: Fast index creation, lower memory during build
# - Cons: Slower queries than HNSW, lower recall, requires dataset-specific tuning
#
# When to use IVFFlat vs HNSW:
# - Use IVFFlat: Development, <100K vectors, rapid prototyping, memory-constrained builds
# - Use HNSW: Production, >100K vectors, query performance critical, ample memory

# IVFFlat Lists: Number of inverted lists (clusters)
# Rule of thumb: sqrt(total_rows) for optimal performance
# Range: 1-10000 (common: 100-10000)
# Examples:
# - 100 for 10K vectors
# - 316 for 100K vectors (sqrt(100000) ≈ 316)
# - 1000 for 1M vectors (sqrt(1000000) = 1000)
# - 3162 for 10M vectors
# More lists = faster queries but slower builds and lower recall
PGVECTOR_IVFFLAT_LISTS=100

# IVFFlat Probes: Number of lists to search during query
# Range: 1-lists (default: 1, recommended: 10-20 for good recall)
# - Lower values (1-5): Fastest queries, lower recall
# - Medium values (10-20): Good balance (recommended)
# - Higher values (50-100): Better recall, slower queries
# - probes=lists: Exact search (defeats purpose of indexing)
# Recommendation: Start with 10, increase if recall is insufficient
PGVECTOR_IVFFLAT_PROBES=10

# ===========================================
# pgvector Extension Configuration
# ===========================================
# Note: The pgvector extension is automatically enabled via database migration
# No additional environment variables are required for basic pgvector functionality
# The Docker image (pgvector/pgvector:pg16) includes pgvector pre-installed
#
# Version Requirements:
# - PostgreSQL: 11+ (recommended: 14+ for best performance)
# - pgvector: 0.5.0+ (recommended: 0.8.0+ for HNSW improvements)
# - Docker: pgvector/pgvector:pg16 (includes PostgreSQL 16 + pgvector 0.8.1)
#
# -------------------------------------------
# PostgreSQL Performance Tuning for pgvector
# -------------------------------------------
# These settings optimize PostgreSQL for vector operations in production
# Configure via Docker command arguments, postgresql.conf, or environment variables
# Recommendations scale with available system RAM
#
# Memory Configuration (16GB RAM system):
# ----------------------------------------
# shared_buffers=4GB
#   Purpose: Main buffer cache for frequently accessed data
#   Formula: 25% of total RAM (not to exceed 8GB on most systems)
#   Impact: Reduces disk I/O, speeds up vector index traversal
#   Range: 128MB-8GB (default: 128MB)
#   Recommendation: 25% of RAM, max 8GB
#
# work_mem=256MB
#   Purpose: Memory for individual query operations (sorting, hashing, joins)
#   Impact: Enables in-memory operations, faster vector comparisons
#   Range: 4MB-2GB (default: 4MB)
#   Recommendation: 64MB-512MB depending on query complexity
#   Note: Allocated per operation, so total usage = work_mem * concurrent operations
#
# maintenance_work_mem=2GB
#   Purpose: Memory for maintenance operations (index creation, VACUUM)
#   Impact: Critical for fast HNSW/IVFFlat index builds
#   Range: 64MB-8GB (default: 64MB)
#   Recommendation: 12.5% of RAM (2GB for 16GB system)
#   Note: Higher values dramatically speed up index creation
#
# effective_cache_size=12GB
#   Purpose: Estimate of disk cache available to PostgreSQL
#   Formula: 75% of total RAM
#   Impact: Query planner optimization, affects index usage decisions
#   Range: 1GB-100GB (default: 4GB)
#   Recommendation: 75% of RAM
#
# Parallelization (8 CPU cores):
# --------------------------------
# max_parallel_workers_per_gather=4
#   Purpose: Number of workers for parallel query execution
#   Impact: Speeds up large vector searches
#   Range: 0-8 (default: 2)
#   Recommendation: 50% of CPU cores (4 for 8-core system)
#
# max_parallel_workers=8
#   Purpose: Total parallel workers for all queries
#   Impact: Overall parallel query capacity
#   Range: 0-16 (default: 8)
#   Recommendation: Equal to CPU core count
#
# max_parallel_maintenance_workers=4
#   Purpose: Workers for parallel index builds
#   Impact: Dramatically speeds up HNSW/IVFFlat index creation
#   Range: 0-8 (default: 2)
#   Recommendation: 50% of CPU cores
#   Note: Critical for large datasets (1M+ vectors)
#
# max_worker_processes=8
#   Purpose: Maximum background worker processes
#   Range: 0-16 (default: 8)
#   Recommendation: Equal to CPU core count
#
# Vector-Specific Settings:
# --------------------------
# hnsw.ef_search=40
#   Purpose: Default ef_search for HNSW index queries (overrides index setting)
#   Impact: Query quality vs speed tradeoff
#   Range: 10-1000 (default: 40)
#   Note: Can be set per-query, this is global default
#
# Connection Settings:
# --------------------
# max_connections=100
#   Purpose: Maximum concurrent database connections
#   Range: 20-1000 (default: 100)
#   Recommendation: Match expected concurrent users/workers
#   Note: Each connection consumes memory
#
# Docker Compose Example (16GB RAM, 8 CPU cores):
# ------------------------------------------------
# services:
#   postgres:
#     image: pgvector/pgvector:pg16
#     command:
#       - "postgres"
#       - "-c" "shared_buffers=4GB"
#       - "-c" "work_mem=256MB"
#       - "-c" "maintenance_work_mem=2GB"
#       - "-c" "effective_cache_size=12GB"
#       - "-c" "max_parallel_workers_per_gather=4"
#       - "-c" "max_parallel_workers=8"
#       - "-c" "max_parallel_maintenance_workers=4"
#       - "-c" "max_worker_processes=8"
#       - "-c" "max_connections=100"
#
# Alternative: postgresql.conf Mount
# -----------------------------------
# For more complex configurations, mount a custom postgresql.conf:
# volumes:
#   - ./postgresql.conf:/etc/postgresql/postgresql.conf
#   - pgdata:/var/lib/postgresql/data
#
# Scaling Guidelines by Dataset Size:
# ------------------------------------
# Small (<100K vectors):
#   - shared_buffers: 1GB
#   - work_mem: 64MB
#   - maintenance_work_mem: 512MB
#   - Index: IVFFlat with 100-316 lists
#
# Medium (100K-1M vectors):
#   - shared_buffers: 2-4GB
#   - work_mem: 128-256MB
#   - maintenance_work_mem: 1-2GB
#   - Index: HNSW with m=16, ef_construction=64
#
# Large (1M-10M vectors):
#   - shared_buffers: 4-8GB
#   - work_mem: 256-512MB
#   - maintenance_work_mem: 2-4GB
#   - Index: HNSW with m=24, ef_construction=128
#   - Consider: Read replicas for query scaling
#
# Extra Large (>10M vectors):
#   - shared_buffers: 8GB (max recommended)
#   - work_mem: 512MB-1GB
#   - maintenance_work_mem: 4-8GB
#   - Index: HNSW with m=32, ef_construction=200
#   - Consider: Partitioning, sharding, or dedicated vector database
#
# Monitoring & Diagnostics:
# --------------------------
# Query performance: SELECT * FROM pg_stat_statements ORDER BY total_time DESC;
# Index usage: SELECT * FROM pg_indexes WHERE tablename = 'vector_embeddings';
# Memory usage: SELECT * FROM pg_stat_database;
# Cache hit ratio: SELECT sum(heap_blks_hit) / (sum(heap_blks_hit) + sum(heap_blks_read)) FROM pg_statio_user_tables;
#
# For detailed tuning guidance, see:
# - docs/pgvector-best-practices.md
# - https://github.com/pgvector/pgvector#performance
# - https://wiki.postgresql.org/wiki/Tuning_Your_PostgreSQL_Server

# ===========================================
# Server Configuration
# ===========================================
SERVER_PORT=3001
SERVER_HOST=0.0.0.0

# ===========================================
# Brain Provider - Claude Code
# ===========================================
# Path to Claude Code CLI (if not in PATH)
CLAUDE_CODE_PATH=claude

# Default timeout for brain execution (ms)
BRAIN_TIMEOUT_MS=300000

# ===========================================
# Brain Provider - Anthropic API (optional)
# ===========================================
ANTHROPIC_API_KEY=your_anthropic_api_key_here

# ===========================================
# Brain Provider - OpenAI API (for embeddings)
# ===========================================
OPENAI_API_KEY=your_openai_api_key_here

# ===========================================
# Vector Search Configuration
# ===========================================
# Master switch to enable/disable all vector search functionality
# When disabled, the application will fall back to traditional text-based search
# Default: true (enabled in production/development, auto-disabled in tests)
VECTOR_SEARCH_ENABLED=true

# -------------------------------------------
# Embedding Provider Configuration
# -------------------------------------------
# Provider for generating vector embeddings from text
# Supported values: 'openai', 'anthropic' (future), 'local' (future)
# Each provider offers different models with varying dimensions and costs
VECTOR_EMBEDDING_PROVIDER=openai

# Embedding Model Selection
# Choose based on your needs for quality, cost, and performance
#
# OpenAI Models:
# - text-embedding-3-small: 1536 dimensions, fast, cost-effective (RECOMMENDED)
#   Cost: $0.00002 per 1K tokens, best balance of quality and cost
# - text-embedding-3-large: 3072 dimensions, highest quality, more expensive
#   Cost: $0.00013 per 1K tokens, best for critical applications
# - text-embedding-ada-002: 1536 dimensions, legacy model (deprecated)
#   Cost: $0.0001 per 1K tokens, superseded by 3-small
#
# Performance characteristics:
# - 3-small: ~0.1s for 1K tokens, excellent for most use cases
# - 3-large: ~0.2s for 1K tokens, marginally better quality
#
# Recommendation: Use text-embedding-3-small unless you need the absolute
# best quality and can justify 6.5x higher costs
VECTOR_EMBEDDING_MODEL=text-embedding-3-small

# Vector Embedding Dimensions
# CRITICAL: This must match your chosen model's output dimensions
# Mismatch will cause runtime errors or database constraint violations
#
# Common dimensions by model:
# - text-embedding-3-small: 1536 dimensions
# - text-embedding-3-large: 3072 dimensions (can be truncated to 1536/768/256)
# - text-embedding-ada-002: 1536 dimensions
# - cohere-embed-english-v3: 1024 dimensions
# - local models: typically 384-768 dimensions
#
# Storage impact: Each vector requires approximately (dimensions * 4 bytes)
# - 1536 dims = ~6 KB per vector
# - 3072 dims = ~12 KB per vector
#
# Note: OpenAI's 3-large model supports dimension truncation (1536/768/256)
# for reduced storage at minimal quality loss
VECTOR_EMBEDDING_DIMENSIONS=1536

# API Key for Embedding Provider
# Optional: If not set, falls back to provider-specific keys:
# - OpenAI provider uses OPENAI_API_KEY
# - Anthropic provider would use ANTHROPIC_API_KEY
# Only set this if you want to use a different API key specifically for embeddings
VECTOR_EMBEDDING_API_KEY=

# Embedding Generation Batch Size
# Number of texts to embed in a single API call
# Range: 1-2048 (OpenAI allows up to 2048 per request)
# Recommendations:
# - Development: 10-50 for faster feedback on errors
# - Production: 100-500 for optimal throughput
# - Large imports: 500-2048 for maximum efficiency
# Note: Larger batches reduce API overhead but increase memory usage
VECTOR_EMBEDDING_BATCH_SIZE=100

# Maximum Retry Attempts for Failed Embedding Requests
# Range: 0-10 (recommended: 3-5)
# Handles transient failures like network issues or rate limits
# Uses exponential backoff between retries (1s, 2s, 4s, etc.)
VECTOR_EMBEDDING_MAX_RETRIES=3

# Timeout for Individual Embedding API Requests (milliseconds)
# Range: 5000-120000 (5 seconds to 2 minutes)
# Recommendations:
# - Small batches (<100): 10000-30000 (10-30 seconds)
# - Large batches (>500): 60000-120000 (1-2 minutes)
# - Development: 30000 (30 seconds) for quick feedback
# Consider: Network latency, batch size, provider response time
VECTOR_EMBEDDING_TIMEOUT_MS=30000

# -------------------------------------------
# Vector Search Configuration
# -------------------------------------------
# Default Distance Metric for Similarity Search
# Supported values: 'cosine', 'l2' (Euclidean), 'inner_product'
#
# Metric characteristics:
# - cosine: Measures angle between vectors, normalized to 0-1
#   Range: 0 (identical) to 1 (opposite)
#   Best for: Most semantic search applications (RECOMMENDED)
#   Use when: Vector magnitude is not meaningful
#
# - l2: Euclidean distance, measures straight-line distance
#   Range: 0 (identical) to infinity
#   Best for: When magnitude matters (e.g., intensity)
#   Use when: Vectors are not normalized
#
# - inner_product: Dot product, faster but requires normalized vectors
#   Range: -1 to 1 (for normalized vectors)
#   Best for: Maximum performance with pre-normalized embeddings
#   Use when: Embeddings are L2-normalized (most are)
#
# Note: OpenAI embeddings are L2-normalized, so cosine ≈ inner_product
# Recommendation: Use 'cosine' for interpretability, 'inner_product' for speed
VECTOR_SEARCH_DEFAULT_METRIC=cosine

# Minimum Similarity Threshold for Search Results
# Range: 0.0-1.0 (for cosine distance)
# - 0.9-1.0: Nearly identical (very strict, few results)
# - 0.7-0.9: Similar concepts (recommended for most cases)
# - 0.5-0.7: Loosely related (may include irrelevant results)
# - <0.5: Very broad matches (not recommended)
# Recommendation: Start with 0.7, adjust based on result quality
VECTOR_SEARCH_DEFAULT_MIN_SIMILARITY=0.7

# Default Number of Results to Return per Search Query
# Range: 1-1000 (recommended: 5-20)
# Recommendations:
# - UI/UX applications: 5-10 for user-facing results
# - RAG (Retrieval Augmented Generation): 10-20 for context
# - Bulk operations: 50-100 for analysis
# Lower values = faster queries, less memory, better UX
VECTOR_SEARCH_DEFAULT_LIMIT=10

# Maximum Allowed Results per Query (hard limit)
# Range: 1-10000 (recommended: 100-1000)
# Protects against queries that request too many results
# Should be significantly higher than DEFAULT_LIMIT
# Very large result sets can cause memory issues and slow responses
VECTOR_SEARCH_MAX_LIMIT=100

# Include Similarity Scores in Search Results by Default
# Values: true | false
# When true: Returns similarity score with each result (recommended)
# When false: Returns only the matching documents (faster)
# Recommendation: true for debugging and quality assessment
VECTOR_SEARCH_INCLUDE_SCORES_BY_DEFAULT=true

# -------------------------------------------
# Vector Index Configuration
# -------------------------------------------
# Name of the vector index table/view
# Default: vector_embeddings (matches the database table name)
# Change only if using custom table schemas
VECTOR_INDEX_NAME=vector_embeddings

# Automatically Create Vector Indexes on Startup
# Values: true | false
# When true: Creates missing indexes automatically (recommended for development)
# When false: Requires manual index creation (recommended for production)
# Note: Index creation can be slow for large datasets (hours for millions of vectors)
# Recommendation: true for development, false for production (use migrations)
VECTOR_INDEX_AUTO_CREATE=true

# IVFFlat Index Lists (for auto-created indexes)
# See PGVECTOR_IVFFLAT_LISTS above for detailed explanation
# This setting is used when VECTOR_INDEX_AUTO_CREATE=true
# Recommendation: Match PGVECTOR_IVFFLAT_LISTS value
VECTOR_INDEX_LISTS=100

# IVFFlat Index Probes (for auto-created indexes)
# See PGVECTOR_IVFFLAT_PROBES above for detailed explanation
# This setting is used when VECTOR_INDEX_AUTO_CREATE=true
# Recommendation: Match PGVECTOR_IVFFLAT_PROBES value
VECTOR_INDEX_PROBES=10

# -------------------------------------------
# Performance & Caching Configuration
# -------------------------------------------
# Enable In-Memory Caching for Embedding Results
# Values: true | false
# When true: Caches embeddings to avoid redundant API calls
# Reduces: API costs, latency, rate limit pressure
# Increases: Memory usage (typically <100MB for 10K cached embeddings)
# Recommendation: true for production, false for development (to test fresh embeddings)
VECTOR_PERFORMANCE_ENABLE_CACHE=true

# Cache Time-To-Live (seconds)
# Range: 60-86400 (1 minute to 24 hours)
# How long to keep cached embeddings before regenerating
# Recommendations:
# - Static content: 86400 (24 hours) or longer
# - Dynamic content: 3600 (1 hour)
# - Frequently updated: 600 (10 minutes)
# Lower values = more API calls, higher cost, fresher results
VECTOR_PERFORMANCE_CACHE_TTL_SECONDS=3600

# Vector Database Connection Pool Size
# Range: 1-100 (recommended: 5-20)
# Number of concurrent database connections for vector operations
# Recommendations:
# - Low traffic: 5-10 connections
# - Medium traffic: 10-20 connections
# - High traffic: 20-50 connections
# Higher values support more concurrent searches but increase memory usage
# Should be less than or equal to DB_POOL_SIZE
VECTOR_PERFORMANCE_POOL_SIZE=10

# ===========================================
# Cost Limits (in cents)
# ===========================================
DEFAULT_DAILY_COST_LIMIT_CENTS=10000
DEFAULT_MONTHLY_COST_LIMIT_CENTS=100000
COST_ALERT_THRESHOLD_PERCENT=80

# ===========================================
# Logging
# ===========================================
LOG_LEVEL=debug
LOG_FORMAT=pretty

# ===========================================
# Security
# ===========================================
# JWT Secret for API authentication (Phase 5)
JWT_SECRET=your_jwt_secret_here

# Encryption key for credentials (32 bytes hex)
ENCRYPTION_KEY=your_32_byte_hex_encryption_key_here
